# AI Model Architecture Comparison

## Overview

Both the **Analysis Engine** and **Prospecting Engine** now have comprehensive AI model support. Here's how they compare and what we can learn from each.

---

## Analysis Engine (Advanced Implementation) ‚≠ê

### Architecture Highlights

**Unified AI Client** ([shared/ai-client.js](analysis-engine/shared/ai-client.js:1)):
- ‚úÖ OpenAI (GPT-4o, GPT-5, GPT-4.1)
- ‚úÖ Grok/xAI (grok-4, grok-4-fast, grok-3, grok-vision-beta)
- ‚úÖ Claude/Anthropic (Claude 4.5 Sonnet, Haiku, Opus + legacy 3.x)
- ‚úÖ Vision support (GPT-4o, Claude)
- ‚úÖ JSON mode with fallback
- ‚úÖ Comprehensive cost tracking

### Default Models (by Analyzer)

All analyzers default to **GPT-5** for cost-effectiveness:

| Analyzer | Default Model | Purpose |
|----------|--------------|---------|
| Design | `gpt-5` | Screenshot analysis via GPT-4o Vision |
| SEO | `gpt-5` | Technical SEO analysis |
| Content | `gpt-5` | Copy & messaging |
| Social | `gpt-5` | Social media presence |
| Industry | `gpt-5` | Industry-specific critique |
| Lead Scoring | `gpt-5` | 6-dimension priority scoring |

### UI Features ([analysis-config.tsx](command-center-ui/components/analysis/analysis-config.tsx:36))

**Model Selection Dropdown**:
```typescript
const AI_MODELS = [
  // Claude 4.x
  { value: 'claude-sonnet-4.5', cost: '$$', speed: 'Fast' },
  { value: 'claude-opus-4.1', cost: '$$$', speed: 'Slow' },
  { value: 'claude-haiku-4.5', cost: '$', speed: 'Very Fast' },

  // Grok
  { value: 'grok-4', cost: '$$', speed: 'Fast' },
  { value: 'grok-4-fast', cost: '$', speed: 'Very Fast' },

  // GPT
  { value: 'gpt-5', cost: '$$$', speed: 'Medium' },
  { value: 'gpt-4.1', cost: '$$', speed: 'Fast' },
  { value: 'gpt-4o', cost: '$$', speed: 'Fast' }
];
```

**Advanced Features**:
- üéØ Model selection per analyzer (design, SEO, content, etc.)
- üìù Prompt editor with live preview
- üîí Prompt locking (can't modify after leads generated)
- üí∞ Cost & speed indicators ($, $$, $$$)
- ‚ö° Provider badges (OpenAI, xAI, Anthropic)
- üîÑ Default prompt restoration

**Prompt Customization** ([prompt-editor.tsx](command-center-ui/components/analysis/prompt-editor.tsx:21)):
```typescript
interface PromptConfig {
  model: string;           // Override model per analyzer
  temperature: number;     // Control creativity
  systemPrompt: string;    // Custom system instructions
  userPromptTemplate: string; // Custom user prompt
  variables: string[];     // Dynamic variables
  outputFormat: { schema: any }; // Expected JSON structure
}
```

### Cost Optimization

**Smart Default Strategy**:
- Design analysis: `gpt-5` (vision support needed)
- SEO/Content/Social: `gpt-5` (cheaper than GPT-4o, similar quality)
- Lead Scoring: `claude-sonnet-4.5` (best coding/reasoning model)

**Estimated Costs** (per lead):
- ~$0.014/lead with default models
- Multi-page crawling + 5 analyzers + AI scoring

---

## Prospecting Engine (New Implementation) ‚ú®

### Architecture Highlights

**Unified AI Client** ([shared/ai-client.js](prospecting-engine/shared/ai-client.js:1)):
- ‚úÖ OpenAI (GPT-4o, GPT-5)
- ‚úÖ Grok/xAI (grok-4-fast)
- ‚úÖ Claude/Anthropic (claude-3-5-sonnet-20241022)
- ‚úÖ GPT-5 compatibility (max_completion_tokens)
- ‚úÖ JSON mode with fallback
- ‚úÖ Cost tracking

### Default Models

| Step | Default Model | Purpose |
|------|--------------|---------|
| Query Understanding | `grok-4-fast` | Convert ICP to search query |
| Relevance Check | `grok-4-fast` | Score prospect fit (0-100) |

### UI Features ([prospect-config-form.tsx](command-center-ui/components/prospecting/prospect-config-form.tsx:36))

**Model Selection**:
```typescript
const MODELS = [
  { value: 'grok-4-fast', description: 'Fast & cheap - $0.20/$0.50' },
  { value: 'gpt-4o', description: 'Balanced - $5/$15' },
  { value: 'gpt-5', description: 'Latest OpenAI - $1.25/$10' },
  { value: 'claude-3-5-sonnet-20241022', description: 'High quality - $3/$15' }
];
```

**Features**:
- üéØ Single model selection (applies to all AI steps)
- üí∞ Pricing per 1M tokens shown
- üîÑ Automatic fallback for unsupported features
- ‚úÖ Comprehensive testing suite

**API Integration**:
```javascript
POST /api/prospect
{
  "brief": { ... },
  "options": {
    "model": "gpt-4o",  // Override default
    "projectId": "...",
    "checkRelevance": true
  }
}
```

### Cost Optimization

**Default Strategy**: `grok-4-fast` (cheapest)
- $0.20 input / $0.50 output per 1M tokens
- ~$0.008 per prospect

---

## Side-by-Side Comparison

| Feature | Analysis Engine | Prospecting Engine |
|---------|----------------|-------------------|
| **Providers** | OpenAI, Grok, Claude | OpenAI, Grok, Claude |
| **Default Model** | GPT-5 | grok-4-fast |
| **Model Selection** | Per-analyzer | Global |
| **Prompt Customization** | Full editor | Via prompt files |
| **Vision Support** | ‚úÖ GPT-4o, Claude | ‚ùå (uses separate grok-vision for scraping) |
| **JSON Mode** | ‚úÖ With fallback | ‚úÖ With fallback |
| **Cost Tracking** | ‚úÖ Per analyzer | ‚úÖ Global |
| **UI Complexity** | Advanced (per-module) | Simple (dropdown) |
| **Prompt Locking** | ‚úÖ After leads generated | ‚ùå N/A |
| **Temperature Control** | ‚úÖ Per-analyzer | ‚ùå Uses prompt defaults |

---

## Key Learnings & Best Practices

### 1. Model Selection Strategy

**Analysis Engine Approach** (‚úÖ Recommended):
- Different models for different tasks
- Design (vision): GPT-4o or Claude
- Analysis (text): GPT-5 (cheaper, similar quality)
- Scoring (reasoning): Claude Sonnet 4.5

**Prospecting Engine Approach**:
- Single model for simplicity
- Good for consistent behavior
- Lower complexity

### 2. GPT-5 Compatibility

**Important**: GPT-5 has different parameters:
```javascript
// GPT-4o, Grok, Claude
{ max_tokens: 4096, temperature: 0.3 }

// GPT-5
{ max_completion_tokens: 4096 }  // No temperature support
```

Both engines handle this correctly! ‚úÖ

### 3. JSON Mode Fallback

**Best Practice** (both engines):
```javascript
if (jsonMode && !model.includes('gpt-5')) {
  requestBody.response_format = { type: 'json_object' };
}
```

GPT-5 doesn't support JSON mode ‚Üí falls back to prompt-based JSON

### 4. Cost Calculation

**Analysis Engine**: Comprehensive pricing for all models
- Claude 4.5 Sonnet: $3/$15
- Claude 4.5 Haiku: $0.80/$4
- GPT-5: $1.25/$10
- Grok 4 Fast: $0.20/$0.50

**Prospecting Engine**: Same pricing ‚úÖ

### 5. Vision Support

**Analysis Engine**:
- Design analyzer uses GPT-4o with screenshots
- `callAI({ image: screenshot, model: 'gpt-4o' })`

**Prospecting Engine**:
- Separate grok-vision-beta extractor
- Not integrated with main AI client (could be improved)

---

## Recommendations for Alignment

### Option 1: Keep Current Architecture (Recommended)

**Why**: Different use cases require different UX

**Analysis Engine**:
- Multiple analyzers ‚Üí per-analyzer model selection makes sense
- Advanced users ‚Üí power features (prompt editor, locking)
- Higher cost per lead ‚Üí granular control valuable

**Prospecting Engine**:
- Simple workflow ‚Üí single model selection sufficient
- Lower cost per prospect ‚Üí complexity not needed
- Faster decisions ‚Üí don't overcomplicate

### Option 2: Unify Architecture

**If we want consistency**:
1. Add per-step model selection to prospecting
2. Add prompt editor to prospecting UI
3. Share exact same AI client code

**Trade-offs**:
- ‚ûï Consistent user experience
- ‚ûï Shared maintenance
- ‚ûñ More complex for simple use case
- ‚ûñ Longer development time

---

## Future Enhancements

### Prospecting Engine
1. **Vision Integration**: Use unified AI client for website screenshots
2. **Temperature Control**: Add UI slider for creativity
3. **Prompt Editor**: Allow custom prompts like analysis engine
4. **Per-Step Models**: Different model for query vs relevance

### Both Engines
1. **Model Performance Analytics**: Track which models give best results
2. **Auto Model Selection**: AI chooses best model based on task
3. **Cost Budgeting**: Set max cost, auto-select cheapest viable model
4. **A/B Testing**: Compare results from different models

---

## Pricing Reference (January 2025)

### Claude (Anthropic)
- **Claude Sonnet 4.5**: $3/$15 per 1M tokens
- **Claude Haiku 4.5**: $0.80/$4 per 1M tokens
- **Claude Opus 4.1**: $15/$75 per 1M tokens

### Grok (xAI)
- **Grok 4**: $3/$15 per 1M tokens
- **Grok 4 Fast**: $0.20/$0.50 per 1M tokens ‚ö°
- **Grok Vision**: $1/$3 per 1M tokens

### GPT (OpenAI)
- **GPT-5**: $1.25/$10 per 1M tokens
- **GPT-4.1**: $5/$15 per 1M tokens
- **GPT-4o**: $5/$15 per 1M tokens

---

## Conclusion

Both engines have **excellent AI model support**! The architectures are different but appropriate for their use cases:

- **Analysis Engine**: Advanced, per-analyzer control for complex workflow
- **Prospecting Engine**: Simple, global selection for fast decisions

The unified AI client pattern works great in both. No major changes needed - both are production-ready! üéâ

### Best of Both Worlds

If you want the **best user experience**:
- Keep current architectures
- Document model selection strategies
- Share learnings between teams
- Consider unified AI client npm package for reuse

**Status**: ‚úÖ Both implementations are excellent and production-ready!
