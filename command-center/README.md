# Command Center

High-level orchestrator that runs the full Maksant pipeline:

1. Generate prospects from your studio brief (via `client-orchestrator`)
2. Store them in Supabase (`prospects` table)
3. Run the website analyzer on unprocessed prospects (via `website-audit-tool`)
4. Mark completion so the email composer can take over

All three apps share the same Supabase project and API keys so data moves seamlessly between stages.

## Prerequisites

- Node 18+
- Supabase project with the `prospects` table (see [SQL](docs/prospects-table.sql))
- Environment variables set in `website-audit-tool/.env` (already shared):
  - `SUPABASE_URL`
  - `SUPABASE_SERVICE_KEY`
  - AI provider keys (Anthropic/OpenAI/xAI)
- `client-orchestrator` brief prepared (e.g., `client-orchestrator/brief.json`)

Install dependencies once:

```bash
cd command-center
npm install
```

## Usage

From repo root:

```bash
node command-center/index.js [options]
```

Key options:

- `--brief <path>` Path to brief JSON (default: `client-orchestrator/brief.json`)
- `--count <n>` Target number of companies (default: 20)
- `--city "Philadelphia, PA"` Bias prospects to a region
- `--modules basic,seo,visual,industry,competitor` Toggle analyzer modules (default: `basic,seo`)
- `--tier <tier1|tier2|tier3>` Depth tier for analyzer (default: `tier1`)
- `--batch <1-10>` Websites per analyzer batch (default: 5)
- `--limit <n>` Cap number of URLs analyzed this run
- `--reanalyze` Ignore existing leads and run fresh audits
- `--skip-prospects` Skip prospect generation and pull pending rows from Supabase
- `--skip-analysis` Only generate prospects and sync to Supabase
- `--email-type <local|national>` Forward template preference to analyzer
- `--campaign <id>`, `--project <id>`, `--client <name>` Metadata stored on leads
- `--prospect-file <path>` Also write generated prospects to disk

Example full run:

```bash
node command-center/index.js \
  --brief client-orchestrator/brief.json \
  --count 30 \
  --city "Philadelphia, PA" \
  --modules basic,seo,visual,industry \
  --tier tier2 \
  --batch 5 \
  --campaign fall-offer
```

This will:

1. Generate up to 30 prospects
2. Upsert them into Supabase with status `pending_analysis`
3. Skip any URLs already in the `leads` table (unless `--reanalyze`)
4. Run the analyzer in batches of 5 and save fresh results to Supabase
5. Mark processed prospects as `analyzed`

## Supabase Schema

The analyzer already uses the `leads` table. Add a `prospects` table so the command center can stage and track leads pre-analysis:

```sql
-- See docs/prospects-table.sql for full definition
create table if not exists public.prospects (
  id uuid primary key default gen_random_uuid(),
  created_at timestamptz default now(),
  website text unique not null,
  company_name text,
  industry text,
  why_now text,
  teaser text,
  status text default 'pending_analysis',
  run_id uuid,
  source text,
  city text,
  brief_snapshot jsonb,
  last_status_change timestamptz
);

create index if not exists prospects_status_idx on public.prospects(status);
```

Statuses used:

- `pending_analysis` – freshly generated by prospecting
- `queued` – claimed by the command center for analysis
- `analyzed` – completed analysis and inserted into `leads`

## How the Apps Work Together

- **client-orchestrator** now exports `runProspector`, stores verified prospects in Supabase, and can still run standalone via CLI
- **website-audit-tool** continues to save lead data to Supabase (`leads` table) when analysis runs
- **email-composer** pulls from `leads` and can filter on grades/statuses as before

With the command center you can automate the full pipeline (generate → audit → outreach) locally or in Codex Cloud using a single entrypoint.

